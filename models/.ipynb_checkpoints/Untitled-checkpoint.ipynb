{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f2e1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from HRE.transformer import Transformer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e66fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from .HRE.transformer import Transformer\n",
    "# from HRE.transformer import Transformer\n",
    "class HRE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_head, n_encoder, n_decoder, n_query, dropout, class_number = 6,activate_regular_restrictions = None):\n",
    "        super(HRE, self).__init__()\n",
    "        \n",
    "        self.in_proj = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers = n_encoder,\n",
    "            num_decoder_layers = n_decoder,\n",
    "            dim_feedforward=3 * hidden_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "     \n",
    "        self.prototype = nn.Embedding(n_query, hidden_dim)  \n",
    "        \n",
    "     \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * n_query, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, class_number),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def former(self, x, transformer, prototype, prompt = None):\n",
    "        b,c,t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.weight.unsqueeze(0).repeat(b, 1, 1)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "        \n",
    "    def forward(self, x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy):\n",
    "        #         [\n",
    "        #             ( 1, 1, 10,),\n",
    "        #              torch.Size([ 1, 1, 150]),\n",
    "        #              torch.Size([,1 1, 150]),\n",
    "        #              (1, 1,86,),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #         ]\n",
    "        \n",
    "        x_arg =  self.in_proj[0](x_arg).permute(0,2,1)\n",
    "        x_code =  self.in_proj[1](x_code).permute(0,2,1)\n",
    "        x_ast =  self.in_proj[2](x_ast).permute(0,2,1)\n",
    "        x_node_type =  self.in_proj[3](x_node_type).permute(0,2,1)\n",
    "        # print( x_arg.shape, x_code.shape, x_ast.shape, x_node_type.shape  )\n",
    "        x_deep = self.in_proj_n[0](x_deep).unsqueeze(1)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total).unsqueeze(1)\n",
    "        x_width = self.in_proj_n[2](x_width).unsqueeze(1)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy).unsqueeze(1) # (1, 64)\n",
    "        \n",
    "        x_feature = torch.cat( [ x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "        print(x_feature)\n",
    "        q = self.former( x_feature, self.transformer, self.prototype)\n",
    "       \n",
    "        x = self.flatten(q)\n",
    "      \n",
    "        y = self.regressor(x)\n",
    "        return y, q\n",
    "\n",
    "class RES(nn.Module):\n",
    "    def __init__(self, in_dim ):\n",
    "        super(RES, self).__init__()\n",
    "        hiddem_dim = in_dim//2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d( in_dim,  hiddem_dim, 3, 1, 1),\n",
    "            nn.BatchNorm1d(hiddem_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d( hiddem_dim,  hiddem_dim, 3, 1, 1),\n",
    "            nn.BatchNorm1d(hiddem_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d( hiddem_dim,  in_dim, 3, 1, 1),\n",
    "            nn.BatchNorm1d(in_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.conv(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class HREC(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_head, n_encoder, n_decoder, n_query, dropout, class_number = 6, activate_regular_restrictions = None):\n",
    "        super(HREC, self).__init__()\n",
    "        \n",
    "        self.in_proj = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                #                 nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "                #                 nn.BatchNorm1d(hidden_dim // 2),\n",
    "                #                 # nn.BatchNorm1d(hidden_dim // 2),\n",
    "                #                 nn.ReLU(),\n",
    "                #                 nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "                #                 # nn.BatchNorm1d(hidden_dim)\n",
    "                nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers = n_encoder,\n",
    "            num_decoder_layers = n_decoder,\n",
    "            dim_feedforward=3 * hidden_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        n_query = n_query * 8\n",
    "        self.prototype = nn.Embedding(n_query, hidden_dim)  \n",
    "        \n",
    "        self.repeat = 4\n",
    "        #         self.dnn= nn.Sequential(\n",
    "        #             RES(2048),\n",
    "        #             nn.Conv1d(2048, 1024,1),\n",
    "        #             nn.BatchNorm1d(1024),\n",
    "        #             nn.ReLU(),\n",
    "\n",
    "        #             RES(1024),\n",
    "        #             nn.Conv1d(1024, 256,1),\n",
    "        #             nn.BatchNorm1d(256),\n",
    "        #             nn.ReLU(),\n",
    "\n",
    "        #             RES(256),\n",
    "        #             nn.Conv1d(256, 64,1),\n",
    "        #             nn.BatchNorm1d(64),\n",
    "        #             nn.ReLU(),\n",
    "        #         )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(n_query*hidden_dim , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "        )\n",
    "        self.c = nn.AvgPool1d(2)\n",
    "        self.hidden_dim = 2048\n",
    "\n",
    "        \n",
    "    def former(self, x, transformer, prototype, prompt = None):\n",
    "        b,c,t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.weight.unsqueeze(0).repeat(b, 1, 1)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "        \n",
    "    def forward(self, x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy):\n",
    "        #         [\n",
    "        #             ( 1, 1, 10,),\n",
    "        #              torch.Size([ 1, 1, 150]),\n",
    "        #              torch.Size([,1 1, 150]),\n",
    "        #              (1, 1,86,),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #         ]\n",
    "        # print(x_arg.shape)\n",
    "        x_arg =  self.in_proj[0](x_arg).permute(0,2,1)\n",
    "        # print(x_arg.shape)\n",
    "        x_code =  self.in_proj[1](x_code).permute(0,2,1)\n",
    "        # print(\"code:\",x_code.shape)\n",
    "        x_ast =  self.in_proj[2](x_ast).permute(0,2,1)\n",
    "        # print(\"ast:\",x_ast.shape)\n",
    "        x_node_type =  self.in_proj[3](x_node_type).permute(0,2,1)\n",
    "        # print(\"x_node:\",x_node_type.shape)\n",
    "        print( x_arg.shape, x_code.shape, x_ast.shape, x_node_type.shape  )\n",
    "        \n",
    "        x_deep = self.in_proj_n[0](x_deep).unsqueeze(1)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total).unsqueeze(1)\n",
    "        x_width = self.in_proj_n[2](x_width).unsqueeze(1)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy).unsqueeze(1) # (1, 64)\n",
    "        \n",
    "        x_feature = torch.cat( [ x_arg, x_code, x_ast, x_node_type], 1)\n",
    "        # print(x_feature.shape)\n",
    "        # torch.cat( [ x_deep, x_node_total, x_width, x_entrop\n",
    "        #         b,t,c = x_feature.shape\n",
    "        #         inp = x_feature.repeat(1,self.repeat,1)\n",
    "        \n",
    "        \n",
    "        x_feature_1  = self.c(x_feature.permute(0,2,1))\n",
    "        x_feature_2 = self.c(x_feature_1)\n",
    "        x_feature_3 = self.c(x_feature_2)\n",
    "        inp = torch.cat([x_feature_3, x_feature_2, x_feature_1, x_feature.permute(0,2,1)],2).permute(0,2,1)\n",
    "        \n",
    "        # b,t,c = inp.shape\n",
    "        # t = self.hidden_dim//t + 1\n",
    "        # inp = inp.repeat(1,t,1)\n",
    "        # inp = inp[:,:self.hidden_dim,:]\n",
    "        \n",
    "        # inp = self.dnn(inp)\n",
    "        # print(inp.shape)\n",
    "        q = self.former( inp, self.transformer, self.prototype)\n",
    "        print(q.shape)\n",
    "        \n",
    "        x = self.flatten(q) \n",
    "        \n",
    "        y = self.regressor(x)\n",
    "        return y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8117430-c912-4ca0-a3e3-20c00d6fea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HREC(1, 64, 8, 1, 2, 6, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "536b0b35-a35b-490d-8c2b-0f7908877cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.in_proj[0][0].bias.to(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2679701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 64]) torch.Size([1, 24, 64]) torch.Size([1, 114, 64]) torch.Size([1, 51, 64])\n",
      "torch.Size([1, 48, 64])\n"
     ]
    }
   ],
   "source": [
    "model = HREC(1, 64, 8, 1, 2, 6, 0.3)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((1,1,10))\n",
    "    x_code =torch.randn((1,1,24))\n",
    "    x_ast = torch.randn((1,1,114))\n",
    "    x_node_type =torch.randn((1,1,51))\n",
    "    x_deep = torch.randn((1,1))\n",
    "    x_node_total = torch.randn((1,1))\n",
    "    x_width = torch.randn((1,1))\n",
    "    x_entropy = torch.randn((1,1))\n",
    "    ans = model(x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b13205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]), 3072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0].shape, 48*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c99d8b-b5bc-48af-ada9-78c6e1e6ff69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510cc66-f406-4ea1-bbfc-24b94f710675",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8054f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature = torch.zeros((1,203,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae669758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 101]), torch.Size([1, 64, 50]), torch.Size([1, 64, 25]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nn.AvgPool1d(2)\n",
    "x_feature_1  = c(x_feature.permute(0,2,1))\n",
    "x_feature_2 = c(x_feature_1)\n",
    "x_feature_3 = c(x_feature_2)\n",
    "\n",
    "x_feature_1.shape, x_feature_2.shape, x_feature_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56fd3b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 379, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x_feature_3, x_feature_2, x_feature_1, x_feature.permute(0,2,1)],2).permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea8d7375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prototype.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4f0c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn= nn.Sequential(\n",
    "    nn.Conv1d(1024, 512,1),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Conv1d(512, 256,1),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Conv1d(256, 64,1),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "055a8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros((1,1024,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efad4338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b72d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RES(nn.Module):\n",
    "    def __init__(self, in_dim ):\n",
    "        super(RES, self).__init__()\n",
    "        hiddem_dim = out_dim//2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d( in_dim,  hiddem_dim, 3, 1, 1),\n",
    "            nn.BatchNorm1d(hiddem_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d( hiddem_dim,  hiddem_dim, 3, 1, 1),\n",
    "            nn.BatchNorm1d(hiddem_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d( hiddem_dim,  in_dim, 3, 1, 1),\n",
    "            nn.BatchNorm1d(in_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.conv(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4e928a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Conv1d( 2,  2, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9cb1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_image = torch.zeros((2,2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b56e0648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(batch_image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba8c06-555f-4f10-a717-f9c0e32c9bd9",
   "metadata": {},
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6036b706-3e2b-4bb2-af68-70a94532f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HREC(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_head, n_encoder, n_decoder, n_query, dropout, class_number = 6, activate_regular_restrictions = None):\n",
    "        super(HREC, self).__init__()\n",
    "        \n",
    "        self.in_proj = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                #                 nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "                #                 nn.BatchNorm1d(hidden_dim // 2),\n",
    "                #                 # nn.BatchNorm1d(hidden_dim // 2),\n",
    "                #                 nn.ReLU(),\n",
    "                #                 nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "                #                 # nn.BatchNorm1d(hidden_dim)\n",
    "                nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.in_proj[0] = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "        \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,6),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        hidden_dim = 1080\n",
    "        self.transformer = Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers = n_encoder,\n",
    "            num_decoder_layers = n_decoder,\n",
    "            dim_feedforward=3 * hidden_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        n_query = n_query \n",
    "        # print(n_query, hidden_dim)\n",
    "        self.prototype = nn.Embedding(n_query, hidden_dim)  \n",
    "        \n",
    "        self.repeat = 4\n",
    "        self.dnn= nn.Sequential(\n",
    "            \n",
    "            nn.Linear( hidden_dim , hidden_dim // 2),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, 256,1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128,1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768 , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.c = nn.AvgPool1d(2)\n",
    "        self.hidden_dim = 2048\n",
    "\n",
    "        \n",
    "    def former(self, x, transformer, prototype, prompt = None):\n",
    "        b,c,t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.weight.unsqueeze(0).repeat(b, 1, 1)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "        \n",
    "    def forward(self, x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy):\n",
    "        #         [\n",
    "        #             ( 1, 1, 10,),\n",
    "        #              torch.Size([ 1, 1, 150]),\n",
    "        #              torch.Size([,1 1, 150]),\n",
    "        #              (1, 1,86,),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #              torch.Size([1, 1]),\n",
    "        #         ]\n",
    "        # print( x_arg.shape, x_code.shape, x_ast.shape, x_node_type.shape  )\n",
    "        x_arg =  self.in_proj[0](x_arg.permute(0,2,1)).permute(0,2,1)\n",
    "       \n",
    "        # x_code =  self.in_proj[1](x_code).permute(0,2,1)\n",
    "        # print(\"code:\",x_code.shape)\n",
    "        # x_ast =  self.in_proj[2](x_ast).permute(0,2,1)\n",
    "        # print(\"ast:\",x_ast.shape)\n",
    "        # x_node_type =  self.in_proj[3](x_node_type).permute(0,2,1)\n",
    "        # print(\"x_node:\",x_node_type.shape)\n",
    "        # print( x_arg.shape, x_code.shape, x_ast.shape, x_node_type.shape  )\n",
    "        x_feature = torch.cat( [ x_arg, x_code, x_ast, x_node_type], 1) # 2 , 4 , 512\n",
    "        # print(x_feature.shape)\n",
    "        \n",
    "        x_deep = self.in_proj_n[0](x_deep).unsqueeze(1)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total).unsqueeze(1)\n",
    "        x_width = self.in_proj_n[2](x_width).unsqueeze(1)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy).unsqueeze(1) # (1, 64)\n",
    "        # print( x_deep.shape, x_node_total.shape, x_width.shape, x_entropy.shape  )\n",
    "        \n",
    "        x_feature_code_2 = torch.cat( [ x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "        # print(x_feature.shape,   x_feature_code_2.shape )\n",
    "        x_feature = torch.cat( [x_feature,   x_feature_code_2], -1 )\n",
    "        print(x_feature.shape)\n",
    "        x_feature_1  = self.c(x_feature)\n",
    "        x_feature_2 = self.c(x_feature_1)\n",
    "        x_feature_3 = self.c(x_feature_2)\n",
    "        inp = torch.cat([x_feature_3, x_feature_2, x_feature_1, x_feature],2)\n",
    "        print( inp.shape )\n",
    "        # b,t,c = inp.shape\n",
    "        # t = self.hidden_dim//t + 1\n",
    "        # inp = inp.repeat(1,t,1)\n",
    "        # inp = inp[:,:self.hidden_dim,:]\n",
    "        \n",
    "        print(inp.shape)\n",
    "        q = self.former( inp, self.transformer, self.prototype)\n",
    "        print(q.shape)\n",
    "        \n",
    "        q = self.dnn(q)\n",
    "        \n",
    "        x = self.flatten(q) \n",
    "        print(x.shape)\n",
    "        # x = torch.cat( [ x, self.flatten(x_feature_code_2)], -1)\n",
    "        y = self.regressor(x)\n",
    "        return y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8729ac2-3f36-4ef3-be08-99e856b45f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3005eb83-661f-4bc9-b6ec-453b3130f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 576])\n",
      "torch.Size([2, 4, 1080])\n",
      "torch.Size([2, 4, 1080])\n",
      "torch.Size([2, 6, 1080])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "model = HREC(in_dim=512, hidden_dim=64, n_head=8, n_encoder=1, n_decoder=2, n_query=6, dropout=0.3)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "    x_ast = torch.randn((2,1,512))\n",
    "    x_node_type =torch.randn((2,1,512))\n",
    "    \n",
    "    x_deep = torch.randn((2,1))\n",
    "    x_node_total = torch.randn((2,1))\n",
    "    x_width = torch.randn((2,1))\n",
    "    x_entropy = torch.randn((2,1))\n",
    "    ans = model(x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e92b86d4-f83d-41cd-aa26-d1b5e57a039b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.diagonal(torch.zeros(2,6,960).permute(0,2,1), dim1=-2, dim2=-1) \n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "73dff25b-0817-4ce1-b190-53ab281c4904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5820fa3e-9db3-4867-aa32-1072bd60d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 960\n",
    "d = nn.Sequential(        \n",
    "    nn.Linear( hidden_dim , hidden_dim // 2),\n",
    "    nn.BatchNorm1d(6),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(hidden_dim // 2, 256,1),\n",
    "    nn.BatchNorm1d(6),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(256, 128,1),\n",
    "    nn.BatchNorm1d(6),\n",
    "    nn.ReLU(),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "77e9d766-9bb2-4aec-9d6c-d8c1833c9bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 128])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(2,6,960)\n",
    "d(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "53bd14be-d97d-4660-9754-a831ff09d8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768+256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a26de-270f-4f00-bef9-03479f3e2c32",
   "metadata": {},
   "source": [
    "# Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a51d3abe-6a75-4d13-9d2a-cecf98727a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HREC_(nn.Module):\n",
    "    def __init__(self, \n",
    "            in_dim, hidden_dim, \n",
    "            n_head, n_encoder, n_decoder, n_query, dropout, \n",
    "            class_number = 6):\n",
    "        super().__init__()\n",
    "    \n",
    "\n",
    "        self.in_proj_head_32 = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "        self.in_proj_head_512 = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=512, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "   \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,6),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        hidden_dim = 960\n",
    "        self.transformer = Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers = n_encoder,\n",
    "            num_decoder_layers = n_decoder,\n",
    "            dim_feedforward=3 * hidden_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        n_query = n_query \n",
    "\n",
    "        self.prototype = nn.Embedding(n_query, hidden_dim)  \n",
    "        \n",
    "        self.repeat = 4\n",
    "        self.dnn= nn.Sequential(\n",
    "            \n",
    "            nn.Linear( hidden_dim , hidden_dim // 2),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, 256,1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128,1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear( 768, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "        )\n",
    "        self.c = nn.AvgPool1d(2)\n",
    "\n",
    "\n",
    "        \n",
    "    def former(self, x, transformer, prototype, prompt = None):\n",
    "        b,c,t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.weight.unsqueeze(0).repeat(b, 1, 1)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b,c,t = x.shape\n",
    "     \n",
    "        if t == 32:\n",
    "            x_feature =  self.in_proj_head_32(x.permute(0,2,1)).permute(0,2,1)\n",
    "        else:\n",
    "            x_feature =  self.in_proj_head_512(x.permute(0,2,1)).permute(0,2,1)\n",
    "    \n",
    "        \n",
    "        x_feature_1  = self.c(x_feature)\n",
    "        x_feature_2 = self.c(x_feature_1)\n",
    "        x_feature_3 = self.c(x_feature_2)\n",
    "        inp = torch.cat([x_feature_3, x_feature_2, x_feature_1, x_feature],2)\n",
    "    \n",
    "       \n",
    "        q = self.former( inp, self.transformer, self.prototype)\n",
    "     \n",
    "        q = self.dnn(q)\n",
    "        \n",
    "     \n",
    "        x = self.flatten(q) \n",
    "        \n",
    "        y = self.regressor(x)\n",
    "\n",
    "        return y, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61863d20-83bb-4384-a78d-84d0d429541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HREC_(in_dim=512, hidden_dim=64, n_head=8, n_encoder=1, n_decoder=2, n_query=6, dropout=0.3)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "  \n",
    "    ans = model(x_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e91199-7fd3-4663-b38f-1f364fb6cbf3",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f8518e5-037f-459c-9595-c190bfc1d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HREC(nn.Module):\n",
    "    def __init__(self, \n",
    "            in_dim, hidden_dim, \n",
    "            n_head, n_encoder, n_decoder, n_query, dropout, \n",
    "            class_number = 6, only_token = \"ALL\"):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.in_proj = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "        \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,6),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        hidden_dim = 960\n",
    "        self.transformer = Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers = n_encoder,\n",
    "            num_decoder_layers = n_decoder,\n",
    "            dim_feedforward=3 * hidden_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        n_query = n_query \n",
    "\n",
    "        self.prototype = nn.Embedding(n_query, hidden_dim)  \n",
    "        \n",
    "        self.repeat = 4\n",
    "        self.dnn= nn.Sequential(\n",
    "            \n",
    "            nn.Linear( hidden_dim , hidden_dim // 2),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, 256,1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128,1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear( 768, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "        )\n",
    "        self.c = nn.AvgPool1d(2)\n",
    "        self.hidden_dim = 2048\n",
    "\n",
    "        \n",
    "    def former(self, x, transformer, prototype, prompt = None):\n",
    "        b,c,t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.weight.unsqueeze(0).repeat(b, 1, 1)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy = x\n",
    "        x_arg =  self.in_proj(x_arg.permute(0,2,1)).permute(0,2,1)\n",
    "        x_feature = torch.cat( [ x_arg, x_code, x_ast, x_node_type], 1) # 2 , 4 , 512\n",
    "        \n",
    "        x_deep = self.in_proj_n[0](x_deep).unsqueeze(1)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total).unsqueeze(1)\n",
    "        x_width = self.in_proj_n[2](x_width).unsqueeze(1)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy).unsqueeze(1) # (1, 64)\n",
    "        x_feature_code_2 = torch.cat( [ x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "       \n",
    "        x_feature_1  = self.c(x_feature)\n",
    "        x_feature_2 = self.c(x_feature_1)\n",
    "        x_feature_3 = self.c(x_feature_2)\n",
    "        inp = torch.cat([x_feature_3, x_feature_2, x_feature_1, x_feature],2)\n",
    "      \n",
    "        q = self.former( inp, self.transformer, self.prototype)\n",
    "   \n",
    "        q = self.dnn(q)\n",
    "      \n",
    "    \n",
    "        x = self.flatten(q) \n",
    "        y = self.regressor(x)\n",
    "\n",
    "        return y, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5712b6f7-0136-4513-be27-8e7f5474f9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 960])\n",
      "torch.Size([2, 6, 960])\n",
      "torch.Size([2, 6, 128])\n"
     ]
    }
   ],
   "source": [
    "model = HREC(in_dim=512, hidden_dim=64, n_head=8, n_encoder=1, n_decoder=2, n_query=6, dropout=0.3)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "    x_ast = torch.randn((2,1,512))\n",
    "    x_node_type =torch.randn((2,1,512))\n",
    "    \n",
    "    x_deep = torch.randn((2,1))\n",
    "    x_node_total = torch.randn((2,1))\n",
    "    x_width = torch.randn((2,1))\n",
    "    x_entropy = torch.randn((2,1))\n",
    "    ans = model([x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baac77-b2ff-4d31-84df-58a19640211f",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a86fcb0-1af0-4bb7-86fa-16960dd7f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, in_dim, class_number = 6):\n",
    "        super(DNN, self).__init__()\n",
    "        hidden_dim = 64\n",
    "        self.in_proj_head_32 = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=64),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "        self.in_proj_head_512 = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=512, out_channels=64),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.dnn= nn.Sequential(\n",
    "            nn.Conv1d(1024, 512,1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(512, 256,1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(256, 64,1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "      \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 64 , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "        )\n",
    "        self.hidden_dim = 1024\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b,c,t = x.shape\n",
    "     \n",
    "        if t == 32:\n",
    "            x_feature =  self.in_proj_head_32(x.permute(0,2,1)).permute(0,2,1)\n",
    "        else:\n",
    "            x_feature =  self.in_proj_head_512(x.permute(0,2,1)).permute(0,2,1)\n",
    "    \n",
    "        b,t,c = x_feature.shape\n",
    "        # print(x_feature.shape)\n",
    "        t = self.hidden_dim//t + 1\n",
    "        inp = x_feature.repeat(1,t,1)\n",
    "        inp = inp[:,:self.hidden_dim,:]\n",
    "        \n",
    "        # print(inp.shape)\n",
    "        q = self.dnn(inp)\n",
    "        x = self.flatten(q)\n",
    "        y = self.regressor(x)\n",
    "        \n",
    "        return y, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e9fdb19-1288-4985-b71f-ae43387e17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(in_dim=1)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "   \n",
    "    ans = model(x_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9aaa066-70c6-427e-8817-a6296cc18806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, in_dim, class_number = 6):\n",
    "        super(DNN, self).__init__()\n",
    "        hidden_dim = 64\n",
    "        self.in_proj = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.dnn= nn.Sequential(\n",
    "            nn.Conv1d(1024, 512,1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(512, 256,1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(256, 64,1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "      \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 64 , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "        )\n",
    "        self.hidden_dim = 1024\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy = x\n",
    "        x_arg =  self.in_proj[0](x_arg).permute(0,2,1)\n",
    "        x_code =  self.in_proj[1](x_code).permute(0,2,1)\n",
    "        x_ast =  self.in_proj[2](x_ast).permute(0,2,1)\n",
    "        x_node_type =  self.in_proj[3](x_node_type).permute(0,2,1)\n",
    "        x_deep = self.in_proj_n[0](x_deep).unsqueeze(1)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total).unsqueeze(1)\n",
    "        x_width = self.in_proj_n[2](x_width).unsqueeze(1)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy).unsqueeze(1) # (1, 64)\n",
    "    \n",
    "        x_feature = torch.cat( [ x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "        # print(x_feature.shape)\n",
    "        b,t,c = x_feature.shape\n",
    "        t = self.hidden_dim//t + 1\n",
    "        inp = x_feature.repeat(1,t,1)\n",
    "        inp = inp[:,:self.hidden_dim,:]\n",
    "        # print(inp.shape)\n",
    "        q = self.dnn(inp)\n",
    "        x = self.flatten(q)\n",
    "        y = self.regressor(x)\n",
    "        \n",
    "        return y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb6035dc-3c2e-427a-a54c-a73048543256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1572, 64])\n",
      "torch.Size([2, 1024, 64])\n"
     ]
    }
   ],
   "source": [
    "model = DNN(in_dim=1)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "    x_ast = torch.randn((2,1,512))\n",
    "    x_node_type =torch.randn((2,1,512))\n",
    "    \n",
    "    x_deep = torch.randn((2,1))\n",
    "    x_node_total = torch.randn((2,1))\n",
    "    x_width = torch.randn((2,1))\n",
    "    x_entropy = torch.randn((2,1))\n",
    "    ans = model([x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9d689-f199-4a5a-b485-a43185486fdd",
   "metadata": {},
   "source": [
    "# ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a955b0-f1f7-42a8-a8b7-a5f389a63c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attn import ChannelAttention, SpatialAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb51ae56-52bf-4180-8402-6de6aeae047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(2,6,1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff944ce-f278-446d-aee2-3a9e625a45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ChannelAttention(1080)\n",
    "b = SpatialAttention(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "486575f6-2331-4635-a845-8fab070c84de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1080])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(a.permute(0,2,1)).permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "388ddefc-b0bb-47cd-995f-38f230536d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b(a.permute(0,2,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb3f02b5-4a95-4841-9082-c03637c318c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HREC(nn.Module):\n",
    "    def __init__(self, \n",
    "            in_dim, hidden_dim, \n",
    "            n_head, n_encoder, n_decoder, n_query, dropout, \n",
    "            class_number = 6, only_token = \"ALL\"):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.in_proj = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "        \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,6),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        hidden_dim = 960\n",
    "        self.transformer = Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers = n_encoder,\n",
    "            num_decoder_layers = n_decoder,\n",
    "            dim_feedforward=3 * hidden_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        n_query = n_query \n",
    "\n",
    "        self.prototype = nn.Embedding(n_query, hidden_dim)  \n",
    "        \n",
    "        self.repeat = 4\n",
    "        self.dnn= nn.Sequential(\n",
    "            \n",
    "            nn.Linear( hidden_dim , hidden_dim // 2),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_dim // 2, 256,1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128,1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear( 768, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "        )\n",
    "        self.c = nn.AvgPool1d(2)\n",
    "        self.hidden_dim = 2048\n",
    "        self.ca = ChannelAttention(960)\n",
    "        self.sa = SpatialAttention(3)\n",
    "\n",
    "        \n",
    "    def former(self, x, transformer, prototype, prompt = None):\n",
    "        b,c,t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.weight.unsqueeze(0).repeat(b, 1, 1)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy = x\n",
    "        x_arg =  self.in_proj(x_arg.permute(0,2,1)).permute(0,2,1)\n",
    "        x_feature = torch.cat( [ x_arg, x_code, x_ast, x_node_type], 1) # 2 , 4 , 512\n",
    "        \n",
    "        x_deep = self.in_proj_n[0](x_deep).unsqueeze(1)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total).unsqueeze(1)\n",
    "        x_width = self.in_proj_n[2](x_width).unsqueeze(1)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy).unsqueeze(1) # (1, 64)\n",
    "        x_feature_code_2 = torch.cat( [ x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "       \n",
    "        x_feature_1  = self.c(x_feature)\n",
    "        x_feature_2 = self.c(x_feature_1)\n",
    "        x_feature_3 = self.c(x_feature_2) # B  4 512\n",
    "        inp = torch.cat([x_feature_3, x_feature_2, x_feature_1, x_feature],2)\n",
    "       \n",
    "        q = self.former( inp, self.transformer, self.prototype)\n",
    "        print(q.shape)\n",
    "        q = q * self.ca(q.permute(0,2,1)).permute(0,2,1) \n",
    "        q = q * self.sa(q)\n",
    "        q = self.dnn(q)\n",
    "        \n",
    "    \n",
    "        x = self.flatten(q) \n",
    "        y = self.regressor(x)\n",
    "\n",
    "        return y, q\n",
    "\n",
    "    def cal(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x1 = x.permute(0,2,1)\n",
    "        self_coef = torch.bmm(x, x1)\n",
    "        return self_coef\n",
    "    \n",
    "    def forward_token(self, x):\n",
    "        x_arg =  self.in_proj[0](x_arg.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "        x_feature_1  = self.c(x_feature)\n",
    "        x_feature_2 = self.c(x_feature_1)\n",
    "        x_feature_3 = self.c(x_feature_2)\n",
    "        inp = torch.cat([x_feature_3, x_feature_2, x_feature_1, x_feature],2)\n",
    "       \n",
    "        q = self.former( inp, self.transformer, self.prototype)\n",
    "        q = self.dnn(q)\n",
    "        \n",
    "    \n",
    "        x = self.flatten(q) \n",
    "        y = self.regressor(x)\n",
    "        \n",
    "        return [y, self.cal(y)], q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cd734c9-c32c-4a33-905b-0fde1b351ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 960])\n"
     ]
    }
   ],
   "source": [
    "model = HREC(in_dim=512, hidden_dim=64, n_head=8, n_encoder=1, n_decoder=2, n_query=6, dropout=0.3)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "    x_ast = torch.randn((2,1,512))\n",
    "    x_node_type =torch.randn((2,1,512))\n",
    "    \n",
    "    x_deep = torch.randn((2,1))\n",
    "    x_node_total = torch.randn((2,1))\n",
    "    x_width = torch.randn((2,1))\n",
    "    x_entropy = torch.randn((2,1))\n",
    "    ans = model([x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71276e2-7961-4461-91c8-4b0dd9290f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
