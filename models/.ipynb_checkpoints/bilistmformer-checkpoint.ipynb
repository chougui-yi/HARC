{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b10d2a8b-afdd-445e-8963-382353419442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from HRE.transformer import Transformer\n",
    "from bigrua import Attention\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, n_hidden, num_classes=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LSTM = nn.LSTM(embedding_dim, n_hidden,\n",
    "                            num_layers=4, batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        # 因为是双向 LSTM, 所以要乘2\n",
    "        self.ffn = nn.Linear(n_hidden * 2, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_hidden_states, context = self.LSTM(inputs)\n",
    "        # lstm_hidden_states = lstm_hidden_states[:, -1, :]\n",
    "        ffn_outputs = self.relu(self.ffn(lstm_hidden_states))\n",
    "\n",
    "        return ffn_outputs, context\n",
    "\n",
    "\n",
    "class CBiLSTMAF(nn.Module):\n",
    "    def __init__(self, in_dim, class_number=6):\n",
    "        super(CBiLSTMAF, self).__init__()\n",
    "        hidden_dim = 64\n",
    "        n_head = 4\n",
    "        n_encoder = 1\n",
    "        n_decoder = 2\n",
    "        dropout = 0.3\n",
    "        # self.in_proj = nn.ModuleList([\n",
    "        #     nn.Sequential(\n",
    "        #         #                 nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "        #         #                 nn.BatchNorm1d(hidden_dim // 2),\n",
    "        #         #                 # nn.BatchNorm1d(hidden_dim // 2),\n",
    "        #         #                 nn.ReLU(),\n",
    "        #         #                 nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "        #         #                 # nn.BatchNorm1d(hidden_dim)\n",
    "        #         nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "        #         nn.BatchNorm1d(hidden_dim // 2),\n",
    "        #         nn.ReLU(),\n",
    "        #         nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "        #         nn.BatchNorm1d(hidden_dim)\n",
    "        #     ) for i in range(4)\n",
    "        # ])\n",
    "        \n",
    "        self.in_proj = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "\n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.bilstm = nn.Sequential(\n",
    "            BiLSTM(in_dim, in_dim, in_dim),\n",
    "        )\n",
    "        self.attn = Attention(in_dim)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            d_model=in_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers=n_encoder,\n",
    "            num_decoder_layers=n_decoder,\n",
    "            dim_feedforward=3 * in_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768 , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.dnn= nn.Sequential(\n",
    "            \n",
    "            nn.Linear( in_dim , in_dim // 2),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_dim // 2, 256,1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128,1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def former(self, x, transformer, prototype, prompt=None):\n",
    "        # print(x.shape, prototype.shape)\n",
    "        b, c, t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.repeat(1, c, t)\n",
    "        # print(q.shape, encode_x.shape)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "\n",
    "    def forward(self, x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy):\n",
    "        x_arg = self.in_proj(x_arg.permute(0,2,1)).permute(0,2,1)\n",
    "        # x_code = self.in_proj[1](x_code)\n",
    "        # x_ast = self.in_proj[2](x_ast)\n",
    "        # x_node_type = self.in_proj[3](x_node_type)\n",
    "        \n",
    "        x_feature_1 = torch.cat([x_arg, x_code, x_ast, x_node_type], 1)\n",
    "        # print( x_arg.shape, x_code.shape, x_ast.shape, x_node_type.shape, x_feature_1.shape )\n",
    "        x_deep = self.in_proj_n[0](x_deep)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total)\n",
    "        x_width = self.in_proj_n[2](x_width)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy)  # (1, 64)\n",
    "\n",
    "        x_feature_2 = torch.cat([x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "        # print( x_feature_1.shape, x_feature_2.shape)\n",
    "        # print(x_feature.shape)\n",
    "        y, context = self.bilstm(x_feature_1)\n",
    "        # print(y.shape, context[0][-1,:,:].unsqueeze(1).shape)\n",
    "        y1, y2 = self.attn(y, context[0][-1, :, :].unsqueeze(1))\n",
    "        #print(y1.shape, y2.shape, y1[:, -1, :].unsqueeze(1).shape)\n",
    "        y_former = self.former(y, self.transformer, y2[:, -1, :].unsqueeze(1))\n",
    "        # print(y_former.shape, x_feature_2.shape)\n",
    "        q = self.dnn(y_former)\n",
    "        q = self.flatten(q)\n",
    "        #print(q.shape, x_feature_2.shape)\n",
    "        \n",
    "        y = self.regressor(torch.cat([q, x_feature_2], -1))\n",
    "        return y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "044014e7-0258-4881-b726-5664789f18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBiLSTMAF(512)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "    x_ast = torch.randn((2,1,512))\n",
    "    x_node_type =torch.randn((2,1,512))\n",
    "    \n",
    "    x_deep = torch.randn((2,1))\n",
    "    x_node_total = torch.randn((2,1))\n",
    "    x_width = torch.randn((2,1))\n",
    "    x_entropy = torch.randn((2,1))\n",
    "    ans = model(x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "37845a07-99e0-4da7-9226-b16e1d40f6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a2f848-be32-43ea-b369-923e210239ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from HRE.transformer import Transformer\n",
    "from bigrua import Attention\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, n_hidden, num_classes=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LSTM = nn.LSTM(embedding_dim, n_hidden,\n",
    "                            num_layers=4, batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        # 因为是双向 LSTM, 所以要乘2\n",
    "        self.ffn = nn.Linear(n_hidden * 2, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_hidden_states, context = self.LSTM(inputs)\n",
    "        # lstm_hidden_states = lstm_hidden_states[:, -1, :]\n",
    "        ffn_outputs = self.relu(self.ffn(lstm_hidden_states))\n",
    "\n",
    "        return ffn_outputs, context\n",
    "\n",
    "\n",
    "class CBiLSTMAF(nn.Module):\n",
    "    def __init__(self, in_dim, class_number=6):\n",
    "        super(CBiLSTMAF, self).__init__()\n",
    "        hidden_dim = 64\n",
    "        n_head = 4\n",
    "        n_encoder = 1\n",
    "        n_decoder = 2\n",
    "        dropout = 0.3\n",
    "        # self.in_proj = nn.ModuleList([\n",
    "        #     nn.Sequential(\n",
    "        #         #                 nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "        #         #                 nn.BatchNorm1d(hidden_dim // 2),\n",
    "        #         #                 # nn.BatchNorm1d(hidden_dim // 2),\n",
    "        #         #                 nn.ReLU(),\n",
    "        #         #                 nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "        #         #                 # nn.BatchNorm1d(hidden_dim)\n",
    "        #         nn.Conv1d(kernel_size=1, in_channels=in_dim, out_channels=hidden_dim // 2),\n",
    "        #         nn.BatchNorm1d(hidden_dim // 2),\n",
    "        #         nn.ReLU(),\n",
    "        #         nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "        #         nn.BatchNorm1d(hidden_dim)\n",
    "        #     ) for i in range(4)\n",
    "        # ])\n",
    "        \n",
    "        self.in_proj = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "\n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_dim)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.bilstm = nn.Sequential(\n",
    "            BiLSTM(in_dim, in_dim, in_dim),\n",
    "        )\n",
    "        self.attn = Attention(in_dim)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            d_model=in_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers=n_encoder,\n",
    "            num_decoder_layers=n_decoder,\n",
    "            dim_feedforward=3 * in_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768 , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.dnn= nn.Sequential(\n",
    "            \n",
    "            nn.Linear( in_dim , in_dim // 2),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_dim // 2, 256,1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128,1),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def former(self, x, transformer, prototype, prompt=None):\n",
    "        # print(x.shape, prototype.shape)\n",
    "        b, c, t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.repeat(1, c, t)\n",
    "        # print(q.shape, encode_x.shape)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy = x\n",
    "        x_arg = self.in_proj(x_arg.permute(0,2,1)).permute(0,2,1)\n",
    "        # x_code = self.in_proj[1](x_code)\n",
    "        # x_ast = self.in_proj[2](x_ast)\n",
    "        # x_node_type = self.in_proj[3](x_node_type)\n",
    "        \n",
    "        x_feature_1 = torch.cat([x_arg, x_code, x_ast, x_node_type], 1)\n",
    "        # print( x_arg.shape, x_code.shape, x_ast.shape, x_node_type.shape, x_feature_1.shape )\n",
    "        x_deep = self.in_proj_n[0](x_deep)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total)\n",
    "        x_width = self.in_proj_n[2](x_width)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy)  # (1, 64)\n",
    "\n",
    "        x_feature_2 = torch.cat([x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "        # print( x_feature_1.shape, x_feature_2.shape)\n",
    "        print(x_feature_1.shape)\n",
    "        y, context = self.bilstm(x_feature_1)\n",
    "        # print(y.shape, context[0][-1,:,:].unsqueeze(1).shape)\n",
    "        y1, y2 = self.attn(y, context[0][-1, :, :].unsqueeze(1))\n",
    "        #print(y1.shape, y2.shape, y1[:, -1, :].unsqueeze(1).shape)\n",
    "        y_former = self.former(y, self.transformer, y2[:, -1, :].unsqueeze(1))\n",
    "        print(y_former.shape, x_feature_2.shape)\n",
    "        q = self.dnn(y_former)\n",
    "        q = self.flatten(q)\n",
    "        print(q.shape, x_feature_2.shape)\n",
    "        \n",
    "        y = self.regressor(torch.cat([q, x_feature_2], -1))\n",
    "        return y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc6e4a0-e650-4539-9983-b59266aa443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 512])\n",
      "torch.Size([2, 4, 512]) torch.Size([2, 256])\n",
      "torch.Size([2, 512]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "model = CBiLSTMAF(512)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "    x_ast = torch.randn((2,1,512))\n",
    "    x_node_type =torch.randn((2,1,512))\n",
    "    \n",
    "    x_deep = torch.randn((2,1))\n",
    "    x_node_total = torch.randn((2,1))\n",
    "    x_width = torch.randn((2,1))\n",
    "    x_entropy = torch.randn((2,1))\n",
    "    ans = model([x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c1d411d-2832-4914-8147-339ffc43e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBiLSTMAF_(nn.Module):\n",
    "    def __init__(self, in_dim, class_number=6):\n",
    "        super().__init__()\n",
    "        hidden_dim = 64\n",
    "        n_head = 4\n",
    "        n_encoder = 1\n",
    "        n_decoder = 2\n",
    "        dropout = 0.3\n",
    "   \n",
    "        self.in_proj_head_32 = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "        self.in_proj_head_512 = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=512, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "     \n",
    "        self.bilstm = nn.Sequential(\n",
    "            BiLSTM(in_dim, in_dim, in_dim),\n",
    "        )\n",
    "        self.attn = Attention(in_dim)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            d_model=in_dim,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers=n_encoder,\n",
    "            num_decoder_layers=n_decoder,\n",
    "            dim_feedforward=3 * in_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.regressor = nn.Sequential(\n",
    "            # nn.Linear(768 , 512),\n",
    "            # nn.Linear(512, 256),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.dnn= nn.Sequential(\n",
    "            \n",
    "            nn.Linear( in_dim , in_dim // 2),\n",
    "            # nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_dim // 2, 256,1),\n",
    "            # nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128,1),\n",
    "            # nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def former(self, x, transformer, prototype, prompt=None):\n",
    "        # print(x.shape, prototype.shape)\n",
    "        b, c, t = x.shape\n",
    "        encode_x = transformer.encoder(x)\n",
    "        if prompt is not None:\n",
    "            encode_x += prompt\n",
    "        q = prototype.repeat(1, c, t)\n",
    "        # print(q.shape, encode_x.shape)\n",
    "        decode_x = transformer.decoder(q, encode_x)\n",
    "        return decode_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,t = x.shape\n",
    "     \n",
    "        if t == 32:\n",
    "            x_feature =  self.in_proj_head_32(x.permute(0,2,1)).permute(0,2,1)\n",
    "        else:\n",
    "            x_feature =  self.in_proj_head_512(x.permute(0,2,1)).permute(0,2,1)\n",
    "    \n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "  \n",
    "        y, context = self.bilstm(x_feature)\n",
    "        # print(y.shape, context[0][-1,:,:].unsqueeze(1).shape)\n",
    "        \n",
    "        y1, y2 = self.attn(y, context[0][-1, :, :].unsqueeze(1))\n",
    "        # print(y1.shape, y2.shape, y1[:, -1, :].unsqueeze(1).shape)\n",
    "        y_former = self.former(y, self.transformer, y2[:, -1, :].unsqueeze(1))\n",
    "        # print(y_former.shape, x_feature_2.shape)\n",
    "        q = self.dnn(y_former)\n",
    "        q = self.flatten(q)\n",
    "        # print(q.shape)\n",
    "        \n",
    "        y = self.regressor(q)\n",
    "        return y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f26beed5-a454-4a9e-baa2-893ad3255413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 512]) torch.Size([2, 1, 512])\n",
      "torch.Size([2, 1, 512]) torch.Size([2, 1, 1]) torch.Size([2, 1, 512])\n",
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "model = CBiLSTMAF_(in_dim=512)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "  \n",
    "    ans = model(x_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f514c8e6-81ed-4b6a-9ab0-275014830cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
