{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40b0315-50ef-4ef1-ae32-a3d70837a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" Applies attention mechanism on the `context` using the `query`.\n",
    "\n",
    "    **Thank you** to IBM for their initial implementation of :class:`Attention`. Here is\n",
    "    their `License\n",
    "    <https://github.com/IBM/pytorch-seq2seq/blob/master/LICENSE>`__.\n",
    "\n",
    "    Args:\n",
    "        dimensions (int): Dimensionality of the query and context.\n",
    "        attention_type (str, optional): How to compute the attention score:\n",
    "\n",
    "            * dot: :math:`score(H_j,q) = H_j^T q`\n",
    "            * general: :math:`score(H_j, q) = H_j^T W_a q`\n",
    "\n",
    "    Example:\n",
    "\n",
    "         >>> attention = Attention(256)\n",
    "         >>> query = torch.randn(5, 1, 256)\n",
    "         >>> context = torch.randn(5, 5, 256)\n",
    "         >>> output, weights = attention(query, context)\n",
    "         >>> output.size()\n",
    "         torch.Size([5, 1, 256])\n",
    "         >>> weights.size()\n",
    "         torch.Size([5, 1, 5])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions, attention_type='general'):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        if attention_type not in ['dot', 'general']:\n",
    "            raise ValueError('Invalid attention type selected.')\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        if self.attention_type == 'general':\n",
    "            self.linear_in = nn.Linear(dimensions, dimensions, bias=False)\n",
    "\n",
    "        self.linear_out = nn.Linear(dimensions * 2, dimensions, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (:class:`torch.FloatTensor` [batch size, output length, dimensions]): Sequence of\n",
    "                queries to query the context.\n",
    "            context (:class:`torch.FloatTensor` [batch size, query length, dimensions]): Data\n",
    "                overwhich to apply the attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            :class:`tuple` with `output` and `weights`:\n",
    "            * **output** (:class:`torch.LongTensor` [batch size, output length, dimensions]):\n",
    "              Tensor containing the attended features.\n",
    "            * **weights** (:class:`torch.FloatTensor` [batch size, output length, query length]):\n",
    "              Tensor containing attention weights.\n",
    "        \"\"\"\n",
    "        batch_size, output_len, dimensions = query.size()\n",
    "        query_len = context.size(1)\n",
    "\n",
    "        if self.attention_type == \"general\":\n",
    "            query = query.reshape(batch_size * output_len, dimensions)\n",
    "            query = self.linear_in(query)\n",
    "            query = query.reshape(batch_size, output_len, dimensions)\n",
    "\n",
    "        # TODO: Include mask on PADDING_INDEX?\n",
    "\n",
    "        # (batch_size, output_len, dimensions) * (batch_size, query_len, dimensions) ->\n",
    "        # (batch_size, output_len, query_len)\n",
    "        attention_scores = torch.bmm(query, context.transpose(1, 2).contiguous())\n",
    "\n",
    "        # Compute weights across every context sequence\n",
    "        attention_scores = attention_scores.view(batch_size * output_len, query_len)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        attention_weights = attention_weights.view(batch_size, output_len, query_len)\n",
    "\n",
    "        # (batch_size, output_len, query_len) * (batch_size, query_len, dimensions) ->\n",
    "        # (batch_size, output_len, dimensions)\n",
    "        mix = torch.bmm(attention_weights, context)\n",
    "\n",
    "        # concat -> (batch_size * output_len, 2*dimensions)\n",
    "        combined = torch.cat((mix, query), dim=2)\n",
    "        combined = combined.view(batch_size * output_len, 2 * dimensions)\n",
    "\n",
    "        # Apply linear_out on every 2nd dimension of concat\n",
    "        # output -> (batch_size, output_len, dimensions)\n",
    "        output = self.linear_out(combined).view(batch_size, output_len, dimensions)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "\n",
    "class BiGRUA(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiGRUA, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        # self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        self.attn = Attention( hidden_size * 2 )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # 双向GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # print(out.shape)\n",
    "        out1, _ = self.attn(out, torch.ones_like(out))\n",
    "        out = out1[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        # out = self.fc(out)\n",
    "        return out, out1\n",
    "\n",
    "class BIGRU_Attention(nn.Module):\n",
    "    def __init__(self, in_dim, class_number = 6):\n",
    "        super(BIGRU_Attention, self).__init__()\n",
    "        hidden_dim = in_dim\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Conv1d(kernel_size=1, in_channels=32, out_channels=hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim)\n",
    "        ) \n",
    "        \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, 64)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.bigrua = nn.Sequential(\n",
    "            BiGRUA( hidden_dim, hidden_dim, 4, 6)\n",
    "        )\n",
    "        self.fc = nn.Linear(in_dim * 2, in_dim)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768 , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy):\n",
    "        x_arg =  self.in_proj(x_arg.permute(0,2,1)).permute(0,2,1)\n",
    "      \n",
    "        # print( x_arg.shape, x_code.shape, x_ast.shape, x_node_type.shape  )\n",
    "        x_deep = self.in_proj_n[0](x_deep).unsqueeze(1)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total).unsqueeze(1)\n",
    "        x_width = self.in_proj_n[2](x_width).unsqueeze(1)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy).unsqueeze(1) # (1, 64)\n",
    "        \n",
    "        x_feature_1 = torch.cat( [ x_arg, x_code, x_ast, x_node_type], 1)\n",
    "        x_feature_2 = torch.cat( [ x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "        # print(x_feature_1.shape, x_feature_2.shape)\n",
    "        y, q = self.bigrua(x_feature_1)\n",
    "        # print(y.shape, q.shape)\n",
    "        q = self.fc(y)\n",
    "        y = self.regressor( torch.cat([q, self.flatten(x_feature_2)], -1) )\n",
    "        return y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9dfed1a-0af4-4877-89b5-d3f91359b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRUA(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiGRUA, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        # self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        self.attn = Attention( hidden_size * 2 )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 初始化隐藏状态\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # 双向GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # print(out.shape)\n",
    "        out1, _ = self.attn(out, torch.ones_like(out))\n",
    "        out = out1[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        # out = self.fc(out)\n",
    "        return out, out1\n",
    "\n",
    "class BIGRU_Attention(nn.Module):\n",
    "    def __init__(self, in_dim, class_number = 6):\n",
    "        super(BIGRU_Attention, self).__init__()\n",
    "        hidden_dim = in_dim\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Conv1d(kernel_size=1, in_channels=32, out_channels=hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(kernel_size=1, in_channels=hidden_dim // 2, out_channels=hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim)\n",
    "        ) \n",
    "        \n",
    "        self.in_proj_n = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, 64)\n",
    "            ) for i in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.bigrua = nn.Sequential(\n",
    "            BiGRUA( hidden_dim, hidden_dim, 4, 6)\n",
    "        )\n",
    "        self.fc = nn.Linear(in_dim * 2, in_dim)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(768 , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy = x\n",
    "        x_arg =  self.in_proj(x_arg.permute(0,2,1)).permute(0,2,1)\n",
    "      \n",
    "        # print( x_arg.shape, x_code.shape, x_ast.shape, x_node_type.shape  )\n",
    "        x_deep = self.in_proj_n[0](x_deep).unsqueeze(1)\n",
    "        x_node_total = self.in_proj_n[1](x_node_total).unsqueeze(1)\n",
    "        x_width = self.in_proj_n[2](x_width).unsqueeze(1)\n",
    "        x_entropy = self.in_proj_n[3](x_entropy).unsqueeze(1) # (1, 64)\n",
    "        \n",
    "        x_feature_1 = torch.cat( [ x_arg, x_code, x_ast, x_node_type], 1)\n",
    "        x_feature_2 = torch.cat( [ x_deep, x_node_total, x_width, x_entropy], 1)\n",
    "        # print(x_feature_1.shape, x_feature_2.shape)\n",
    "        y, q = self.bigrua(x_feature_1)\n",
    "        # print(y.shape, q.shape)\n",
    "        q = self.fc(y)\n",
    "        y = self.regressor( torch.cat([q, self.flatten(x_feature_2)], -1) )\n",
    "        return y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075d2cdb-1234-4600-83bc-3b2f176aea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BIGRU_Attention(512)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "    x_ast = torch.randn((2,1,512))\n",
    "    x_node_type =torch.randn((2,1,512))\n",
    "    \n",
    "    x_deep = torch.randn((2,1))\n",
    "    x_node_total = torch.randn((2,1))\n",
    "    x_width = torch.randn((2,1))\n",
    "    x_entropy = torch.randn((2,1))\n",
    "    ans = model([x_arg, x_code, x_ast, x_node_type, x_deep, x_node_total, x_width, x_entropy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5004db3f-51bd-4b42-a89f-e2dfbb912208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a1bf077-4fef-44b4-a53c-bf2b36d82250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BIGRU_Attention_(nn.Module):\n",
    "    def __init__(self, in_dim, class_number = 6):\n",
    "        super(BIGRU_Attention_, self).__init__()\n",
    "        hidden_dim = in_dim\n",
    "        self.in_proj_head_32 = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=32, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "        self.in_proj_head_512 = nn.Sequential(\n",
    "                nn.Conv1d(kernel_size=1, in_channels=512, out_channels=512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "            ) \n",
    "        \n",
    "        self.bigrua = nn.Sequential(\n",
    "            BiGRUA( hidden_dim, hidden_dim, 4, 6)\n",
    "        )\n",
    "        self.fc = nn.Linear(in_dim * 2, in_dim)\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            # nn.Linear(768 , 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Linear(64, class_number),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        b,c,t = x.shape\n",
    "     \n",
    "        if t == 32:\n",
    "            x_feature =  self.in_proj_head_32(x.permute(0,2,1)).permute(0,2,1)\n",
    "        else:\n",
    "            x_feature =  self.in_proj_head_512(x.permute(0,2,1)).permute(0,2,1)\n",
    "    \n",
    "        y, q = self.bigrua(x_feature)\n",
    "        # print(y.shape, q.shape)\n",
    "        q = self.fc(y)\n",
    "        # print(q.shape)\n",
    "        y = self.regressor( q )\n",
    "        return y, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "480be5f9-1630-44b6-afa0-b1a4230931a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BIGRU_Attention_(512)\n",
    "with torch.no_grad():\n",
    "    x_arg = torch.randn((2,1,32))\n",
    "    x_code =torch.randn((2,1,512))\n",
    "    \n",
    "    ans = model(x_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14042230-46e7-46f4-b872-0edc815dc2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3df829-cab8-4852-a40c-f644463d8919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
